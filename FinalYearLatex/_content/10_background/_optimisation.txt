Numerical optimisation is process of applying recursive algorithms
to an objective function in order to find the optimal set of arguments that 
result in the function's maximum or minimum value. [cite numerical optimisation]

\begin{equation}\label{eq:optimisation}
    \mathbf{x}^{*} = \underset{\mathbf{x}}{\mathrm{arg\:min}}\:\: f(\mathbf{x})
\end{equation}

An objective function is proposed as `$\mathbf{x}^{*}$ \emph{is equal to the 
argument of the minimum value of} $f(\mathbf{x})$'. Equation \ref{eq:optimisation} is 
the mathematical notation for this statement, where $\mathbf{x}^{*}$ is the optimal column 
matrix that minimises $f(\mathbf{x})$. The objective function can also be subject to equality and 
inequality constraints of the form;

\begin{align}
    c(\mathbf{x})           \leq    0         \\
    c_{eq}(\mathbf{x})      =       0         \\
    A       \mathbf{x}      \leq    b         \\
    A_{eq}  \mathbf{x}      =       b_{eq}    \\
    lb      \leq  \mathbf{x} \leq ub
\end{align}

where $c(\mathbf{x})$ and $c_{eq}(\mathbf{x})$ are non-linear constraints, while the 
matrices $A$ and $A_{eq}$ form linear constraints with $b$ and $b_{eq}$. $lb$ is a 
column matrix that forms the lower bound of $\mathbf{x}$, while $ub$ is the column matrix
that forms the upper bound of $\mathbf{x}$.

Numerical optimisation is used to solve the inverse kinematics problem, where, given 
a desired end effector pose, the column matrix of joint variables that achieve the desired pose 
is returned.

\begin{equation}
    \mathbf{q^{*}} = \mathbf{k}^{-1}(\mathbf{x_e})
\end{equation}


